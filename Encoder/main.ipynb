{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import trimesh\n",
    "from functools import partial\n",
    "import ssl\n",
    "import pickle\n",
    "from Solvers import SolverEmbedding,Loss\n",
    "from Models.EncoderModels import TextEncoder, ShapeEncoder\n",
    "from config import cfg\n",
    "from dataEmbedding.dataEmbedding import Read_Load_BuildBatch\n",
    "from dataEmbedding.dataEmbeddingLoader import GenerateDataLoader,check_dataset,collate_embedding\n",
    "from dataEmbedding.generateEmbedding import build_embeedings_CWGAN\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device=torch.device(\"cuda\")\n",
    "else:\n",
    "    device=torch.device(\"cpu\")\n",
    "\n",
    "device=cfg.DEVICE\n",
    "print(device)\n",
    "PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "#for mac os fix \n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanData=Read_Load_BuildBatch(cfg.EMBEDDING_VOXEL_FOLDER,cfg.EMBEDDING_BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of list is : 14.0\n",
      "Mean: 16.307937873199133\n"
     ]
    }
   ],
   "source": [
    "stanData.wordlens.sort()\n",
    "mid = len(stanData.wordlens) // 2\n",
    "res = (stanData.wordlens[mid] + stanData.wordlens[~mid]) / 2\n",
    "print(\"Median of list is : \" + str(res))\n",
    "print(\"Mean: \"+ str(sum(stanData.wordlens)/len(stanData.wordlens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion={\n",
    "        'walker': Loss.RoundTripLoss(device=device),\n",
    "        'visit': Loss.AssociationLoss(device=device),\n",
    "        #'metric': Loss.SmoothedMetricLoss(device=device)\n",
    "        #'metric': Loss.InstanceMetricLoss()\n",
    "        #'metric': Loss.TripletLoss()\n",
    "        'metric':Loss.NPairLoss()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ShapeModel=ShapeEncoder()\n",
    "ShapeModel=ShapeModel.to(device)\n",
    "TextModel=TextEncoder(len(stanData.dict_word2idx))\n",
    "TextModel=TextModel.to(device)\n",
    "optimizer = torch.optim.Adam(list(ShapeModel.parameters()) + list(TextModel.parameters()), lr=cfg.EMBEDDING_LR, weight_decay=cfg.EMBEDDING_WEIGHT_DC)\n",
    "history=SolverEmbedding.Solver(TextModel,ShapeModel,stanData,optimizer,criterion,cfg.EMBEDDING_BATCH_SIZE,'online',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m matrix1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(shape1)\n\u001b[1;32m      6\u001b[0m matrix2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(shape2)\n\u001b[0;32m----> 8\u001b[0m pos_sum\u001b[39m=\u001b[39m(matrix1\u001b[39m*\u001b[39;49mmatrix2\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m))\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Transpose the second matrix along the first two dimensions (64, 128, 63)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m#transposed_matrix2 = matrix2.transpose(1, 2)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m# Print the shape of the result\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m#print(result.shape)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "shape1 = (64, 128)\n",
    "shape2 = (64, 128)\n",
    "\n",
    "# Create random matrices with the specified shapes\n",
    "matrix1 = torch.randn(shape1)\n",
    "matrix2 = torch.randn(shape2)\n",
    "\n",
    "result_vector = (matrix1 @ matrix2.transpose(0,1)).diagonal()\n",
    "\n",
    "# Transpose the second matrix along the first two dimensions (64, 128, 63)\n",
    "#transposed_matrix2 = matrix2.transpose(1, 2)\n",
    "\n",
    "# Initialize an empty result matrix\n",
    "#result = torch.zeros((shape1[0], shape2[1]))\n",
    "\n",
    "# Perform the element-wise multiplications and summation\n",
    "#for i in range(shape1[0]):\n",
    "#    result[i] = torch.matmul(matrix1[i],transposed_matrix2[i]) -3\n",
    "\n",
    "# Print the shape of the result\n",
    "#print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4] starting...\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86b7429276c4967a2a8e680c6211127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  3,   5,   7,  ..., 123, 125, 127],\n",
      "        [  1,   5,   7,  ..., 123, 125, 127],\n",
      "        [  1,   3,   7,  ..., 123, 125, 127],\n",
      "        ...,\n",
      "        [  1,   3,   5,  ..., 121, 125, 127],\n",
      "        [  1,   3,   5,  ..., 121, 123, 127],\n",
      "        [  1,   3,   5,  ..., 121, 123, 125]])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 128])\n",
      "torch.Size([64, 63, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history\u001b[39m.\u001b[39;49mtrain(cfg\u001b[39m.\u001b[39;49mEMBEDDING_EPOCH_NR,stanData\u001b[39m.\u001b[39;49mdict_idx2word)\n",
      "File \u001b[0;32m~/szkola/3D_Gen/Encoder/Solvers/SolverEmbedding.py:114\u001b[0m, in \u001b[0;36mSolver.train\u001b[0;34m(self, epoch, idx_word)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mfor\u001b[39;00m i,(_,labels,texts , _, shapes) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataloader[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m    113\u001b[0m     pbar\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m--> 114\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(shapes, texts, labels)\n\u001b[1;32m    116\u001b[0m     \u001b[39mif\u001b[39;00m cfg\u001b[39m.\u001b[39mEMBEDDING_SHAPE_ENCODER:\n\u001b[1;32m    118\u001b[0m         train_log[\u001b[39m'\u001b[39m\u001b[39mtotal_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(losses[\u001b[39m'\u001b[39m\u001b[39mtotal_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/szkola/3D_Gen/Encoder/Solvers/SolverEmbedding.py:197\u001b[0m, in \u001b[0;36mSolver.forward\u001b[0;34m(self, shapes, texts, labels)\u001b[0m\n\u001b[1;32m    193\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape_encoder(shapes)\n\u001b[1;32m    195\u001b[0m t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_encoder(texts)\n\u001b[0;32m--> 197\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(s, t, shape_labels, text_labels)\n\u001b[1;32m    199\u001b[0m \u001b[39mreturn\u001b[39;00m losses\n",
      "File \u001b[0;32m~/szkola/3D_Gen/Encoder/Solvers/SolverEmbedding.py:222\u001b[0m, in \u001b[0;36mSolver.compute_loss\u001b[0;34m(self, s, t, s_labels, t_labels)\u001b[0m\n\u001b[1;32m    218\u001b[0m     walker_loss_sts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion[\u001b[39m'\u001b[39m\u001b[39mwalker\u001b[39m\u001b[39m'\u001b[39m](s, t, p_target_s)\n\u001b[1;32m    219\u001b[0m     visit_loss_st \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion[\u001b[39m'\u001b[39m\u001b[39mvisit\u001b[39m\u001b[39m'\u001b[39m](s, t)\n\u001b[0;32m--> 222\u001b[0m metric_loss_tt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcriterion[\u001b[39m'\u001b[39;49m\u001b[39mmetric\u001b[39;49m\u001b[39m'\u001b[39;49m](t,t_labels)\n\u001b[1;32m    224\u001b[0m \u001b[39mif\u001b[39;00m cfg\u001b[39m.\u001b[39mEMBEDDING_SHAPE_ENCODER:\n\u001b[1;32m    225\u001b[0m     s_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mBoolTensor([[\u001b[39m1\u001b[39m], [\u001b[39m0\u001b[39m]])\u001b[39m.\u001b[39mrepeat(batch_size \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, cfg\u001b[39m.\u001b[39mEMBEDDING_DIM)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/szkola/3D_Gen/Encoder/Solvers/Loss.py:157\u001b[0m, in \u001b[0;36mNPairLoss.forward\u001b[0;34m(self, embeddings, t_labels)\u001b[0m\n\u001b[1;32m    153\u001b[0m     losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_pair_loss(anchors, positives, negatives) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml2_reg \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ml2_loss(anchors, positives)\n\u001b[1;32m    155\u001b[0m     \u001b[39mreturn\u001b[39;00m losses\n\u001b[0;32m--> 157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_n_pairs\u001b[39m(\u001b[39mself\u001b[39m,labels):\n\u001b[1;32m    158\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m    Get index of n-pairs and n-negatives\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m    :param labels: label vector of mini-batch\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m    :return: A tuple of n_pairs (n, 2)\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m                    and n_negatives (n, n-1)\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     n_pairs\u001b[39m=\u001b[39m[]\n",
      "File \u001b[0;32m~/szkola/3D_Gen/Encoder/Solvers/Loss.py:205\u001b[0m, in \u001b[0;36mn_pair_loss\u001b[0;34m(self, anchors, positives, negatives)\u001b[0m\n\u001b[1;32m    202\u001b[0m transposed_negatives = negatives.transpose(1, 2)\n\u001b[1;32m    204\u001b[0m print(pos_sum.shape)\n\u001b[0;32m--> 205\u001b[0m # Initialize an empty result matrix\n\u001b[1;32m    206\u001b[0m result = torch.zeros((anchors[0], negatives[1]))\n\u001b[1;32m    208\u001b[0m # Perform the element-wise multiplications and summation\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "history.train(cfg.EMBEDDING_EPOCH_NR,stanData.dict_idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233/233 [00:52<00:00,  4.44it/s]\n",
      "100%|██████████| 30/30 [00:03<00:00,  9.25it/s]\n",
      "100%|██████████| 30/30 [00:27<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "loader={'train' : GenerateDataLoader(stanData.train,stanData.data_dir,stanData.dict_word2idx),\n",
    "        'test' : GenerateDataLoader(stanData.test,stanData.data_dir,stanData.dict_word2idx),\n",
    "        'val':GenerateDataLoader(stanData.val,stanData.data_dir,stanData.dict_word2idx)}\n",
    "\n",
    "dataloader = {\n",
    "            'train': DataLoader(\n",
    "                loader['train'], \n",
    "                batch_size=cfg.EMBEDDING_BATCH_SIZE * 2,              \n",
    "                drop_last=check_dataset(loader['train'], cfg.EMBEDDING_BATCH_SIZE * 2),\n",
    "                collate_fn=collate_embedding,\n",
    "                num_workers=4\n",
    "            ),\n",
    "            'val': DataLoader(\n",
    "                loader['val'], \n",
    "                batch_size=cfg.EMBEDDING_BATCH_SIZE*2,\n",
    "                collate_fn=collate_embedding,\n",
    "                num_workers=4\n",
    "            ),\n",
    "            'test': DataLoader(\n",
    "                loader['test'], \n",
    "                batch_size=cfg.EMBEDDING_BATCH_SIZE*2,\n",
    "                collate_fn=collate_embedding\n",
    "                #num_workers=2\n",
    "            )\n",
    "    }   \n",
    "build_embeedings_CWGAN(cfg.EMBEDDING_TEXT_MODELS_PATH,TextEncoder(len(stanData.dict_word2idx)),dataloader,cfg.EMBEDDING_SAVE_PATH,cfg.DEVICE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
