{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import trimesh\n",
    "from functools import partial\n",
    "import ssl\n",
    "import pickle\n",
    "from config import cfg\n",
    "import torch.optim as optim\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device=torch.device(\"cuda\")\n",
    "else:\n",
    "    device=torch.device(\"cpu\")\n",
    "\n",
    "from DataLoader.GANloader import GANLoader,check_dataset,collate_embedding\n",
    "from DataLoader.GanDataGen import GANDataGenerator\n",
    "from Models.Generator import Generator16,Generator32\n",
    "from Models.Discriminator import Discriminator16,Discriminator32\n",
    "\n",
    "from Solvers.SolverGAN import SolverGAN\n",
    "device=cfg.DEVICE\n",
    "#print(device)\n",
    "PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "#for mac os fix \n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "torch.manual_seed(cfg.SEED)\n",
    "np.random.seed(cfg.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_Data=GANDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator=Generator32().to(cfg.DEVICE)\n",
    "critic=Discriminator32().to(cfg.DEVICE)\n",
    "\n",
    "d_optimizer = optim.Adam(critic.parameters(), lr=cfg.GAN_LR, weight_decay=cfg.GAN_WEIGHT_DECAY)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=cfg.GAN_LR, weight_decay=cfg.GAN_WEIGHT_DECAY)\n",
    "optimizer={'disc':d_optimizer,\n",
    "           'gen':g_optimizer}\n",
    "\n",
    "history=SolverGAN(GAN_Data,generator,critic,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39f9b7c543b4729a90566ff0f9ccb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:608: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Convolution.cpp:1004.)\n",
      "  return F.conv3d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake time \n",
      "11.64777398109436\n",
      "real time \n",
      "6.268564939498901\n",
      "real mis time\n",
      "5.002378940582275\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brysiakp/szkola/3D_Gen/WGAN/Solvers/SolverGAN.py:224: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:491.)\n",
      "  print(gp_critic['logits'].grad)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/brysiakp/szkola/3D_Gen/WGAN/main.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/brysiakp/szkola/3D_Gen/WGAN/main.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history\u001b[39m.\u001b[39;49mtrain(\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/szkola/3D_Gen/WGAN/Solvers/SolverGAN.py:173\u001b[0m, in \u001b[0;36mSolverGAN.train\u001b[0;34m(self, genSteps)\u001b[0m\n\u001b[1;32m    170\u001b[0m     gp_text_data \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    171\u001b[0m start\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 173\u001b[0m losses\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalculateLossDisc(d_out_fake_match,d_out_real_match,d_out_real_mismatch,d_out_gp,gp_text_data,gp_shape_data)\n\u001b[1;32m    175\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscriminator\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    177\u001b[0m losses[\u001b[39m'\u001b[39m\u001b[39md_loss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/szkola/3D_Gen/WGAN/Solvers/SolverGAN.py:228\u001b[0m, in \u001b[0;36mSolverGAN.calculateLossDisc\u001b[0;34m(self, fake_critic, mat_critic, mis_critic, gp_critic, gp_text, gp_shape)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mprint\u001b[39m(gradient_gp_t\u001b[39m.\u001b[39mgrad)\n\u001b[1;32m    224\u001b[0m \u001b[39mprint\u001b[39m(gp_critic[\u001b[39m'\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mgrad)\n\u001b[0;32m--> 228\u001b[0m gradients_dtext \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(outputs\u001b[39m=\u001b[39;49mgp_critic[\u001b[39m'\u001b[39;49m\u001b[39mlogits\u001b[39;49m\u001b[39m'\u001b[39;49m], inputs\u001b[39m=\u001b[39;49mgradient_gp_t,create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    231\u001b[0m gradients_dshape \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(outputs\u001b[39m=\u001b[39mgp_critic[\u001b[39m'\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m'\u001b[39m], inputs\u001b[39m=\u001b[39mgradient_gp_s, create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    233\u001b[0m gradients_dshape_reshaped \u001b[39m=\u001b[39m gradients_dshape\u001b[39m.\u001b[39mview(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:288\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[1;32m    283\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m grad_outputs_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[39mlen\u001b[39m(t_outputs))\n\u001b[0;32m--> 288\u001b[0m grad_outputs_ \u001b[39m=\u001b[39m _make_grads(t_outputs, grad_outputs_, is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched)\n\u001b[1;32m    290\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:88\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m     87\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 88\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[1;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "history.train(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
